{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbfdf1fa4a1b34c6aa89886bb276dfd26",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# Entity Recognition using Conditional Random Fields\n",
    "### Task: Train a machine learning model using the provided training dataset to identify adverse events and SSI from drug reviews. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install -U 'scikit-learn<0.24'\n",
    "\n",
    "#!{sys.executable} -m pip install -U 'scikit-learn<0.24'\n",
    "#!{sys.executable} -m pip install sklearn_crfsuite\n",
    "\n",
    "#!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "import re, sys, os, unicodedata, string, nltk, operator, itertools, collections, sklearn, tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection  import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from pymetamap import MetaMap\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "mm = MetaMap.get_instance('./opt/public_mm/bin/metamap21')\n"
   ]
  },
  {
   "source": [
    "## Read in the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/szoriac/OneDrive/Michigan/=WN 2021/LHS 712/Assignment 3 CRF LSTM\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"/Users/szoriac/OneDrive/Michigan/=WN 2021/LHS 712/Assignment 3 CRF LSTM\") \n",
    "print(os.getcwd())\n",
    "\n",
    "row_id_text, texts = read_file('./REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('./REVIEW_LABELSEQ.txt')\n",
    "\n",
    "#For this demo, let's just use the first 100 sentences \n",
    "texts = texts\n",
    "tags = tags\n"
   ]
  },
  {
   "source": [
    "## Derive POS dictionary From Brown Corpus "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordtags = dict(nltk.ConditionalFreqDist((w.lower(), t) \n",
    "#        for w, t in nltk.corpus.brown.tagged_words(tagset = 'universal')))\n",
    "\n",
    "#wordtagsnofreq = {}\n",
    "#for k, v in wordtags.items():\n",
    "#    wordtagsnofreq[k] = dict(v)\n"
   ]
  },
  {
   "source": [
    "## Derive POS dictionary from Own Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts\n",
    "\n",
    "loweredsent = []\n",
    "for sent in texts:\n",
    "    loweredword = []\n",
    "    for word in sent:\n",
    "        x = word\n",
    "        x = re.sub('[^A-Za-z0-9]+', ' ', x) \n",
    "        loweredword.append(x)\n",
    "    loweredsent.append(loweredword)\n",
    "\n",
    "taggedsent = [nltk.pos_tag(sent, tagset='universal') for sent in loweredsent]\n",
    "flattaggedsent = [item for sublist in taggedsent for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusdictfromtraining = dict(nltk.ConditionalFreqDist((w.lower(), t) \n",
    "        for w, t in flattaggedsent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusdictfromtrainingnofreq = {}\n",
    "for k, v in corpusdictfromtraining.items():\n",
    "    corpusdictfromtrainingnofreq[k] = dict(v)\n",
    "\n",
    "#masterposdict = corpusdictfromtrainingnofreq"
   ]
  },
  {
   "source": [
    "## Merge dictionaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergebtoa(a, b, path=None):\n",
    "    \"merges b into a\"\n",
    "    if path is None: path = []\n",
    "    for key in b:\n",
    "        if key in a:\n",
    "            if isinstance(a[key], dict) and isinstance(b[key], dict):\n",
    "                merge(a[key], b[key], path + [str(key)])\n",
    "            elif a[key] == b[key]:\n",
    "                pass # same leaf value\n",
    "            else:\n",
    "                raise Exception('Conflict at %s' % '.'.join(path + [str(key)]))\n",
    "        else:\n",
    "            a[key] = b[key]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'PRON': 3074, 'ADV': 5, 'NOUN': 168, 'ADJ': 60, 'VERB': 35, 'ADP': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "#masterposdict = mergebtoa(masterposdict, wordtagsnofreq)\n",
    "corpusdictfromtrainingnofreq['i']\n"
   ]
  },
  {
   "source": [
    "## Get set of sickness and symptom hyponyms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sickness = wn.synset('sickness.n.01')\n",
    "typesOfsickness= list(set([w for s in sickness.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "\n",
    "sicktype = []\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in nltk.pos_tag(typesOfsickness):\n",
    "    x = token.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    lemma = lemma_function.lemmatize(x, tag_map[tag[0]])\n",
    "    y = lemma.split(' ')\n",
    "    z = token.replace(\"_\", \" \").replace(\"-\", \" \").split(' ')\n",
    "    for item in y:\n",
    "        lem = lemma_function.lemmatize(item, tag_map[tag[0]])\n",
    "        sicktype.append(lem)   \n",
    "    for item in z:\n",
    "        sicktype.append(item)\n",
    "\n",
    "sicknesses = set(sicktype)\n",
    "\n",
    "\n",
    "\n",
    "symptom = wn.synset('symptom.n.01')\n",
    "typesOfsymptom= list(set([w for s in symptom.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "\n",
    "symptype = []\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in nltk.pos_tag(typesOfsymptom):\n",
    "    x = token.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    lemma = lemma_function.lemmatize(x, tag_map[tag[0]])\n",
    "    y = lemma.split(' ')\n",
    "    z = token.replace(\"_\", \" \").replace(\"-\", \" \").split(' ')\n",
    "    for item in y:\n",
    "        lem = lemma_function.lemmatize(item, tag_map[tag[0]])\n",
    "        symptype.append(lem)   \n",
    "    for item in z:\n",
    "        symptype.append(item)\n",
    "\n",
    "symptoms = set(symptype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filtered_sicknesses = [word for word in sicknesses if word not in stopwords.words('english')]\n",
    "filtered_symptoms = [word for word in symptoms if word not in stopwords.words('english')]"
   ]
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(word, index, tokenizedtext):\n",
    "    strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "\n",
    "    dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "    \n",
    "\n",
    "    features = {\n",
    "        'tokenizedtext': tokenizedtext,\n",
    "        'position': index,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.stripascii()': strippedasciilower,\n",
    "        'word.strippunc()': strippedpunc,\n",
    "        'word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "        'word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "        'word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "        'word[-3:]': strippedpunc[-3:],\n",
    "        'word[-2:]': strippedpunc[-2:],\n",
    "        'word[:2]': strippedpunc[:2],\n",
    "        'word[:3]': strippedpunc[:3],\n",
    "        'word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "        'word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "        'word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "        'word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "        'word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "        'word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "        'word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "        'word.POSfirst()': list(dictionary.items())[0][0],\n",
    "        'word.POSlast()': list(dictionary.items())[-1][0],\n",
    "        'sent.sentimentpos()': float(sia.polarity_scores(\" \".join(tokenizedtext))['pos']),\n",
    "        'sent.sentimentneg()': float(sia.polarity_scores(\" \".join(tokenizedtext))['neg']),\n",
    "        'sent.sentimentneu()': float(sia.polarity_scores(\" \".join(tokenizedtext))['neu']),\n",
    "\n",
    "        'word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "        'word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "        'word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "        'word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "        'word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu']),\n",
    "\n",
    "        'word.isdepress()': 'yes' if strippedpunc in ['depression', 'depressed'] else 'no',\n",
    "        'word.isanxious()': 'yes' if strippedpunc in ['anxiety', 'anxious', 'worry', 'worried'] else 'no',\n",
    "        'word.issuicide()': 'yes' if strippedpunc in ['suicide', 'suicidal', 'kill'] else 'no',\n",
    "        'word.isinsomnia()': 'yes' if strippedpunc in ['insomnia', 'insomniac', 'sleep'] else 'no',\n",
    "        'word.isheadtired()': 'yes' if strippedpunc in ['tired', 'fatigue', 'migraine'] else 'no',\n",
    "        'word.isdelusion()': 'yes' if strippedpunc in ['delusional', 'delusion'] else 'no',\n",
    "        'word.isanger()': 'yes' if strippedpunc in ['anger', 'angry', 'fury', 'furious', 'mad'] else 'no',\n",
    "        'word.isbulimia()': 'yes' if strippedpunc in ['bulimia', 'bulimic'] else 'no',\n",
    "        'word.isafraid()': 'yes' if strippedpunc in ['fear', 'afraid'] else 'no',\n",
    "        'word.isdisorder()': 'yes' if strippedpunc in ['disorder'] else 'no',\n",
    "        'word.isbipolar()': 'yes' if strippedpunc in ['bipolar'] else 'no'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    if index > 0:\n",
    "        word = tokenizedtext[index-1]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '-1:word.lower()': word.lower(),\n",
    "            '-1:word.stripascii()': strippedasciilower,\n",
    "            '-1:word.strippunc()': strippedpunc,\n",
    "            '-1word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '-1word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '-1word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '-1word[-3:]': strippedpunc[-3:],\n",
    "            '-1word[-2:]': strippedpunc[-2:],\n",
    "            '-1word[:2]': strippedpunc[:2],\n",
    "            '-1word[:3]': strippedpunc[:3],\n",
    "            '-1word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '-1word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '-1word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '-1word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '-1word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '-1word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '-1word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '-1word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '-1word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '-1word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '-1word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '-1word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '-1word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '-1word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "\n",
    "    if index > 1:\n",
    "        word = tokenizedtext[index-2]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '-2:word.lower()': word.lower(),\n",
    "            '-2:word.stripascii()': strippedasciilower,\n",
    "            '-2:word.strippunc()': strippedpunc,\n",
    "            '-2word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '-2word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '-2word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '-2word[-3:]': strippedpunc[-3:],\n",
    "            '-2word[-2:]': strippedpunc[-2:],\n",
    "            '-2word[:2]': strippedpunc[:2],\n",
    "            '-2word[:3]': strippedpunc[:3],\n",
    "            '-2word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '-2word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '-2word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '-2word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '-2word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '-2word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '-2word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '-2word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '-2word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '-2word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '-2word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '-2word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '-2word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '-2word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "    \n",
    "    if index < len(tokenizedtext)-1:\n",
    "        word = tokenizedtext[index+1]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '+1:word.lower()': word.lower(),\n",
    "            '+1:word.stripascii()': strippedasciilower,\n",
    "            '+1:word.strippunc()': strippedpunc,\n",
    "            '+1word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '+1word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '+1word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '+1word[-3:]': strippedpunc[-3:],\n",
    "            '+1word[-2:]': strippedpunc[-2:],\n",
    "            '+1word[:2]': strippedpunc[:2],\n",
    "            '+1word[:3]': strippedpunc[:3],\n",
    "            '+1word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '+1word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '+1word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '+1word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '+1word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '+1word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '+1word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '+1word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '+1word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '+1word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '+1word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '+1word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '+1word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '+1word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "    if index < len(tokenizedtext)-2:\n",
    "        \n",
    "        word = tokenizedtext[index+2]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '+2:word.lower()': word.lower(),\n",
    "            '+2:word.stripascii()': strippedasciilower,\n",
    "            '+2:word.strippunc()': strippedpunc,\n",
    "            '+2word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '+2word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '+2word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '+2word[-3:]': strippedpunc[-3:],\n",
    "            '+2word[-2:]': strippedpunc[-2:],\n",
    "            '+2word[:2]': strippedpunc[:2],\n",
    "            '+2word[:3]': strippedpunc[:3],\n",
    "            '+2word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '+2word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '+2word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '+2word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '+2word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '+2word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '+2word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '+2word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '+2word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '+2word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '+2word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '+2word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '+2word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '+2word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "def text2features(text):\n",
    "    tokenizedtext = list(text)\n",
    "    return [word2features(i, index, tokenizedtext) for index,i in enumerate(text)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Define input and split data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4744.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af5b714cbe654584ba300a882179b0e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = [text2features(text) for text in tqdm(texts)]\n",
    "y = tags\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.2)\n"
   ]
  },
  {
   "source": [
    "## Fit and predict CRF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 3795/3795 [00:15<00:00, 249.85it/s]\n",
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 165947\n",
      "Seconds required: 2.694\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 300\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=1.09  loss=77071.38 active=164465 feature_norm=0.50\n",
      "Iter 2   time=0.34  loss=62585.27 active=152981 feature_norm=0.42\n",
      "Iter 3   time=0.29  loss=45018.38 active=121286 feature_norm=0.49\n",
      "Iter 4   time=0.29  loss=42603.59 active=161857 feature_norm=0.52\n",
      "Iter 5   time=0.33  loss=35659.00 active=164292 feature_norm=0.61\n",
      "Iter 6   time=0.29  loss=35200.75 active=142622 feature_norm=0.73\n",
      "Iter 7   time=0.29  loss=32517.24 active=145659 feature_norm=0.77\n",
      "Iter 8   time=0.32  loss=31688.98 active=152241 feature_norm=0.76\n",
      "Iter 9   time=0.30  loss=30829.81 active=150621 feature_norm=0.80\n",
      "Iter 10  time=0.31  loss=29925.01 active=152110 feature_norm=0.88\n",
      "Iter 11  time=0.30  loss=28844.72 active=152352 feature_norm=0.92\n",
      "Iter 12  time=0.30  loss=27180.19 active=153091 feature_norm=1.05\n",
      "Iter 13  time=0.30  loss=24122.55 active=152402 feature_norm=1.35\n",
      "Iter 14  time=0.30  loss=19608.72 active=154965 feature_norm=2.08\n",
      "Iter 15  time=0.31  loss=17514.78 active=154142 feature_norm=2.61\n",
      "Iter 16  time=0.32  loss=15369.71 active=152361 feature_norm=3.19\n",
      "Iter 17  time=0.33  loss=12560.97 active=147287 feature_norm=4.25\n",
      "Iter 18  time=0.35  loss=11110.86 active=144787 feature_norm=4.98\n",
      "Iter 19  time=0.30  loss=10278.77 active=144907 feature_norm=5.86\n",
      "Iter 20  time=0.34  loss=9677.09  active=144860 feature_norm=6.25\n",
      "Iter 21  time=0.36  loss=9358.60  active=144042 feature_norm=6.75\n",
      "Iter 22  time=0.33  loss=8926.66  active=141558 feature_norm=7.44\n",
      "Iter 23  time=0.34  loss=8309.03  active=134529 feature_norm=8.68\n",
      "Iter 24  time=0.33  loss=7374.20  active=127502 feature_norm=11.10\n",
      "Iter 25  time=0.33  loss=6307.25  active=122855 feature_norm=15.69\n",
      "Iter 26  time=0.34  loss=5883.99  active=121107 feature_norm=16.46\n",
      "Iter 27  time=0.35  loss=5507.34  active=120400 feature_norm=17.10\n",
      "Iter 28  time=0.33  loss=5117.03  active=117805 feature_norm=16.98\n",
      "Iter 29  time=0.33  loss=4827.75  active=114463 feature_norm=18.12\n",
      "Iter 30  time=0.32  loss=4492.41  active=116527 feature_norm=18.13\n",
      "Iter 31  time=0.31  loss=4352.27  active=116773 feature_norm=18.64\n",
      "Iter 32  time=0.34  loss=4175.96  active=116648 feature_norm=19.27\n",
      "Iter 33  time=0.35  loss=3828.02  active=114051 feature_norm=21.44\n",
      "Iter 34  time=1.03  loss=3725.69  active=111168 feature_norm=23.19\n",
      "Iter 35  time=0.35  loss=3393.56  active=111620 feature_norm=25.30\n",
      "Iter 36  time=0.34  loss=3236.10  active=111993 feature_norm=26.55\n",
      "Iter 37  time=0.29  loss=2937.96  active=110894 feature_norm=29.25\n",
      "Iter 38  time=0.30  loss=2626.11  active=109057 feature_norm=32.85\n",
      "Iter 39  time=0.30  loss=2480.16  active=105665 feature_norm=35.58\n",
      "Iter 40  time=0.30  loss=2307.37  active=105541 feature_norm=37.12\n",
      "Iter 41  time=0.31  loss=2191.63  active=104008 feature_norm=38.55\n",
      "Iter 42  time=0.29  loss=1998.44  active=98330 feature_norm=41.90\n",
      "Iter 43  time=1.21  loss=1951.05  active=98297 feature_norm=42.78\n",
      "Iter 44  time=0.29  loss=1857.93  active=97962 feature_norm=43.74\n",
      "Iter 45  time=0.30  loss=1763.85  active=95544 feature_norm=45.16\n",
      "Iter 46  time=0.29  loss=1664.89  active=91873 feature_norm=48.58\n",
      "Iter 47  time=1.72  loss=1654.07  active=92763 feature_norm=48.65\n",
      "Iter 48  time=0.30  loss=1622.59  active=92915 feature_norm=48.83\n",
      "Iter 49  time=0.30  loss=1591.80  active=91205 feature_norm=49.23\n",
      "Iter 50  time=0.29  loss=1528.99  active=87241 feature_norm=50.67\n",
      "Iter 51  time=0.29  loss=1526.74  active=86436 feature_norm=50.80\n",
      "Iter 52  time=0.32  loss=1481.98  active=87138 feature_norm=50.90\n",
      "Iter 53  time=0.33  loss=1462.13  active=86543 feature_norm=50.91\n",
      "Iter 54  time=0.33  loss=1426.70  active=81180 feature_norm=50.55\n",
      "Iter 55  time=0.30  loss=1406.41  active=80741 feature_norm=50.36\n",
      "Iter 56  time=0.29  loss=1393.74  active=80920 feature_norm=50.27\n",
      "Iter 57  time=0.34  loss=1381.99  active=80242 feature_norm=50.07\n",
      "Iter 58  time=0.30  loss=1362.06  active=78579 feature_norm=49.56\n",
      "Iter 59  time=0.29  loss=1343.46  active=78192 feature_norm=49.28\n",
      "Iter 60  time=0.30  loss=1329.95  active=77915 feature_norm=49.20\n",
      "Iter 61  time=0.30  loss=1318.06  active=76070 feature_norm=49.06\n",
      "Iter 62  time=0.29  loss=1299.82  active=73566 feature_norm=48.91\n",
      "Iter 63  time=0.31  loss=1288.39  active=72779 feature_norm=48.86\n",
      "Iter 64  time=0.30  loss=1280.24  active=72209 feature_norm=48.87\n",
      "Iter 65  time=0.30  loss=1269.76  active=70213 feature_norm=48.87\n",
      "Iter 66  time=0.32  loss=1255.94  active=67027 feature_norm=48.98\n",
      "Iter 67  time=0.36  loss=1247.40  active=67068 feature_norm=48.97\n",
      "Iter 68  time=0.36  loss=1241.79  active=66897 feature_norm=48.97\n",
      "Iter 69  time=0.37  loss=1235.03  active=65784 feature_norm=48.91\n",
      "Iter 70  time=0.32  loss=1226.34  active=64034 feature_norm=48.83\n",
      "Iter 71  time=0.30  loss=1219.81  active=63581 feature_norm=48.79\n",
      "Iter 72  time=0.31  loss=1214.57  active=63527 feature_norm=48.75\n",
      "Iter 73  time=0.30  loss=1209.54  active=62754 feature_norm=48.67\n",
      "Iter 74  time=0.30  loss=1201.24  active=61216 feature_norm=48.54\n",
      "Iter 75  time=0.29  loss=1196.89  active=60481 feature_norm=48.41\n",
      "Iter 76  time=0.30  loss=1190.95  active=60417 feature_norm=48.39\n",
      "Iter 77  time=0.30  loss=1186.40  active=59688 feature_norm=48.30\n",
      "Iter 78  time=0.32  loss=1180.12  active=58454 feature_norm=48.14\n",
      "Iter 79  time=0.29  loss=1176.59  active=57868 feature_norm=48.03\n",
      "Iter 80  time=0.34  loss=1170.61  active=58087 feature_norm=48.00\n",
      "Iter 81  time=0.29  loss=1167.37  active=57832 feature_norm=47.95\n",
      "Iter 82  time=0.33  loss=1163.06  active=57247 feature_norm=47.88\n",
      "Iter 83  time=0.33  loss=1158.77  active=56267 feature_norm=47.76\n",
      "Iter 84  time=0.31  loss=1154.55  active=56133 feature_norm=47.72\n",
      "Iter 85  time=0.33  loss=1150.85  active=55891 feature_norm=47.69\n",
      "Iter 86  time=0.31  loss=1147.94  active=55648 feature_norm=47.66\n",
      "Iter 87  time=0.31  loss=1144.44  active=55114 feature_norm=47.61\n",
      "Iter 88  time=0.29  loss=1139.94  active=54173 feature_norm=47.56\n",
      "Iter 89  time=0.30  loss=1136.94  active=53714 feature_norm=47.53\n",
      "Iter 90  time=0.31  loss=1133.21  active=53646 feature_norm=47.54\n",
      "Iter 91  time=0.29  loss=1130.88  active=53373 feature_norm=47.54\n",
      "Iter 92  time=0.29  loss=1127.82  active=52894 feature_norm=47.52\n",
      "Iter 93  time=0.32  loss=1124.52  active=52192 feature_norm=47.50\n",
      "Iter 94  time=0.29  loss=1121.67  active=52136 feature_norm=47.51\n",
      "Iter 95  time=0.30  loss=1119.13  active=52150 feature_norm=47.50\n",
      "Iter 96  time=0.32  loss=1116.90  active=51927 feature_norm=47.49\n",
      "Iter 97  time=0.29  loss=1113.45  active=51319 feature_norm=47.45\n",
      "Iter 98  time=0.29  loss=1109.67  active=50463 feature_norm=47.42\n",
      "Iter 99  time=0.32  loss=1106.88  active=50450 feature_norm=47.40\n",
      "Iter 100 time=0.29  loss=1104.96  active=50507 feature_norm=47.41\n",
      "Iter 101 time=0.33  loss=1102.84  active=50288 feature_norm=47.38\n",
      "Iter 102 time=0.32  loss=1100.56  active=50039 feature_norm=47.36\n",
      "Iter 103 time=0.31  loss=1098.31  active=49365 feature_norm=47.29\n",
      "Iter 104 time=0.29  loss=1096.28  active=49283 feature_norm=47.30\n",
      "Iter 105 time=0.43  loss=1094.10  active=49071 feature_norm=47.25\n",
      "Iter 106 time=0.36  loss=1092.51  active=48843 feature_norm=47.24\n",
      "Iter 107 time=0.34  loss=1090.44  active=48409 feature_norm=47.17\n",
      "Iter 108 time=0.39  loss=1088.37  active=47909 feature_norm=47.10\n",
      "Iter 109 time=0.31  loss=1086.53  active=47715 feature_norm=47.01\n",
      "Iter 110 time=0.34  loss=1084.61  active=47800 feature_norm=47.01\n",
      "Iter 111 time=0.32  loss=1083.19  active=47603 feature_norm=46.97\n",
      "Iter 112 time=0.32  loss=1081.44  active=47352 feature_norm=46.93\n",
      "Iter 113 time=0.33  loss=1078.96  active=46873 feature_norm=46.85\n",
      "Iter 114 time=0.32  loss=1078.04  active=46686 feature_norm=46.83\n",
      "Iter 115 time=0.34  loss=1075.74  active=46790 feature_norm=46.81\n",
      "Iter 116 time=0.34  loss=1074.59  active=46785 feature_norm=46.79\n",
      "Iter 117 time=0.33  loss=1072.63  active=46317 feature_norm=46.74\n",
      "Iter 118 time=0.33  loss=1071.06  active=45600 feature_norm=46.68\n",
      "Iter 119 time=0.34  loss=1068.86  active=45626 feature_norm=46.63\n",
      "Iter 120 time=0.34  loss=1067.63  active=45781 feature_norm=46.63\n",
      "Iter 121 time=0.33  loss=1066.44  active=45628 feature_norm=46.60\n",
      "Iter 122 time=0.34  loss=1064.54  active=45265 feature_norm=46.56\n",
      "Iter 123 time=0.34  loss=1062.83  active=44565 feature_norm=46.50\n",
      "Iter 124 time=0.34  loss=1061.16  active=44738 feature_norm=46.49\n",
      "Iter 125 time=0.33  loss=1059.99  active=44821 feature_norm=46.46\n",
      "Iter 126 time=0.33  loss=1058.97  active=44766 feature_norm=46.45\n",
      "Iter 127 time=0.32  loss=1057.10  active=44326 feature_norm=46.40\n",
      "Iter 128 time=0.34  loss=1056.25  active=43975 feature_norm=46.39\n",
      "Iter 129 time=0.35  loss=1054.25  active=44036 feature_norm=46.35\n",
      "Iter 130 time=0.34  loss=1053.29  active=44079 feature_norm=46.35\n",
      "Iter 131 time=0.30  loss=1052.17  active=43899 feature_norm=46.34\n",
      "Iter 132 time=0.28  loss=1050.35  active=43397 feature_norm=46.32\n",
      "Iter 133 time=0.28  loss=1049.59  active=42761 feature_norm=46.25\n",
      "Iter 134 time=0.31  loss=1047.83  active=42924 feature_norm=46.27\n",
      "Iter 135 time=0.29  loss=1046.97  active=42876 feature_norm=46.25\n",
      "Iter 136 time=0.28  loss=1046.09  active=42853 feature_norm=46.24\n",
      "Iter 137 time=0.31  loss=1044.16  active=42278 feature_norm=46.20\n",
      "Iter 138 time=0.29  loss=1043.41  active=41885 feature_norm=46.18\n",
      "Iter 139 time=0.30  loss=1042.14  active=41985 feature_norm=46.17\n",
      "Iter 140 time=0.31  loss=1041.62  active=42097 feature_norm=46.18\n",
      "Iter 141 time=0.30  loss=1040.66  active=42056 feature_norm=46.17\n",
      "Iter 142 time=0.31  loss=1039.80  active=41910 feature_norm=46.16\n",
      "Iter 143 time=0.33  loss=1039.15  active=41613 feature_norm=46.13\n",
      "Iter 144 time=0.32  loss=1038.69  active=41596 feature_norm=46.15\n",
      "Iter 145 time=0.33  loss=1037.50  active=41461 feature_norm=46.14\n",
      "Iter 146 time=0.33  loss=1036.71  active=41510 feature_norm=46.14\n",
      "Iter 147 time=0.31  loss=1036.03  active=41356 feature_norm=46.13\n",
      "Iter 148 time=0.29  loss=1035.23  active=41105 feature_norm=46.12\n",
      "Iter 149 time=0.62  loss=1034.30  active=41055 feature_norm=46.11\n",
      "Iter 150 time=0.31  loss=1033.91  active=40823 feature_norm=46.10\n",
      "Iter 151 time=0.29  loss=1032.89  active=40799 feature_norm=46.09\n",
      "Iter 152 time=0.32  loss=1032.28  active=40821 feature_norm=46.10\n",
      "Iter 153 time=0.29  loss=1031.49  active=40781 feature_norm=46.10\n",
      "Iter 154 time=0.30  loss=1030.99  active=40744 feature_norm=46.10\n",
      "Iter 155 time=0.30  loss=1030.37  active=40589 feature_norm=46.10\n",
      "Iter 156 time=0.30  loss=1030.01  active=40398 feature_norm=46.10\n",
      "Iter 157 time=0.28  loss=1029.38  active=40242 feature_norm=46.09\n",
      "Iter 158 time=0.31  loss=1028.64  active=40306 feature_norm=46.10\n",
      "Iter 159 time=0.31  loss=1027.94  active=40208 feature_norm=46.09\n",
      "Iter 160 time=0.30  loss=1027.42  active=40212 feature_norm=46.10\n",
      "Iter 161 time=0.30  loss=1026.92  active=40140 feature_norm=46.09\n",
      "Iter 162 time=0.30  loss=1026.46  active=40083 feature_norm=46.10\n",
      "Iter 163 time=0.34  loss=1025.97  active=40012 feature_norm=46.09\n",
      "Iter 164 time=0.33  loss=1025.55  active=40003 feature_norm=46.10\n",
      "Iter 165 time=0.33  loss=1025.12  active=39905 feature_norm=46.10\n",
      "Iter 166 time=0.32  loss=1024.63  active=39860 feature_norm=46.12\n",
      "Iter 167 time=0.34  loss=1024.16  active=39831 feature_norm=46.12\n",
      "Iter 168 time=0.34  loss=1023.70  active=39840 feature_norm=46.14\n",
      "Iter 169 time=0.32  loss=1023.30  active=39759 feature_norm=46.14\n",
      "Iter 170 time=0.34  loss=1022.88  active=39781 feature_norm=46.16\n",
      "Iter 171 time=0.36  loss=1022.49  active=39679 feature_norm=46.16\n",
      "Iter 172 time=0.34  loss=1022.18  active=39667 feature_norm=46.19\n",
      "Iter 173 time=0.35  loss=1021.76  active=39597 feature_norm=46.18\n",
      "Iter 174 time=0.35  loss=1021.39  active=39590 feature_norm=46.20\n",
      "Iter 175 time=0.32  loss=1020.97  active=39541 feature_norm=46.20\n",
      "Iter 176 time=0.31  loss=1020.68  active=39516 feature_norm=46.22\n",
      "Iter 177 time=0.32  loss=1020.24  active=39445 feature_norm=46.22\n",
      "Iter 178 time=0.29  loss=1019.96  active=39405 feature_norm=46.24\n",
      "Iter 179 time=0.31  loss=1019.54  active=39291 feature_norm=46.23\n",
      "Iter 180 time=0.30  loss=1019.35  active=39272 feature_norm=46.26\n",
      "Iter 181 time=0.30  loss=1018.86  active=39177 feature_norm=46.25\n",
      "Iter 182 time=0.29  loss=1018.64  active=39114 feature_norm=46.28\n",
      "Iter 183 time=0.30  loss=1018.17  active=39054 feature_norm=46.28\n",
      "Iter 184 time=0.30  loss=1017.94  active=39010 feature_norm=46.30\n",
      "Iter 185 time=0.29  loss=1017.50  active=38907 feature_norm=46.30\n",
      "Iter 186 time=0.30  loss=1017.28  active=38869 feature_norm=46.32\n",
      "Iter 187 time=0.31  loss=1016.86  active=38774 feature_norm=46.32\n",
      "Iter 188 time=0.29  loss=1016.65  active=38715 feature_norm=46.35\n",
      "Iter 189 time=0.30  loss=1016.21  active=38666 feature_norm=46.35\n",
      "Iter 190 time=0.32  loss=1015.99  active=38574 feature_norm=46.37\n",
      "Iter 191 time=0.30  loss=1015.59  active=38491 feature_norm=46.37\n",
      "Iter 192 time=0.32  loss=1015.27  active=38454 feature_norm=46.39\n",
      "Iter 193 time=0.31  loss=1014.94  active=38398 feature_norm=46.39\n",
      "Iter 194 time=0.29  loss=1014.68  active=38398 feature_norm=46.41\n",
      "Iter 195 time=0.29  loss=1014.34  active=38332 feature_norm=46.41\n",
      "Iter 196 time=0.34  loss=1014.05  active=38272 feature_norm=46.43\n",
      "Iter 197 time=0.29  loss=1013.66  active=38245 feature_norm=46.43\n",
      "Iter 198 time=0.32  loss=1013.37  active=38209 feature_norm=46.45\n",
      "Iter 199 time=0.31  loss=1013.02  active=38127 feature_norm=46.44\n",
      "Iter 200 time=0.34  loss=1012.73  active=38142 feature_norm=46.46\n",
      "Iter 201 time=0.30  loss=1012.43  active=38060 feature_norm=46.45\n",
      "Iter 202 time=0.32  loss=1012.13  active=38098 feature_norm=46.46\n",
      "Iter 203 time=0.30  loss=1011.85  active=38006 feature_norm=46.46\n",
      "Iter 204 time=0.29  loss=1011.53  active=37971 feature_norm=46.47\n",
      "Iter 205 time=0.28  loss=1011.24  active=37875 feature_norm=46.47\n",
      "Iter 206 time=0.30  loss=1010.89  active=37836 feature_norm=46.48\n",
      "Iter 207 time=0.30  loss=1010.63  active=37748 feature_norm=46.48\n",
      "Iter 208 time=0.33  loss=1010.34  active=37708 feature_norm=46.50\n",
      "Iter 209 time=0.33  loss=1010.00  active=37640 feature_norm=46.49\n",
      "Iter 210 time=0.32  loss=1009.72  active=37625 feature_norm=46.51\n",
      "Iter 211 time=0.31  loss=1009.34  active=37536 feature_norm=46.51\n",
      "Iter 212 time=0.30  loss=1009.11  active=37438 feature_norm=46.53\n",
      "Iter 213 time=0.29  loss=1008.72  active=37409 feature_norm=46.53\n",
      "Iter 214 time=0.29  loss=1008.51  active=37381 feature_norm=46.54\n",
      "Iter 215 time=0.29  loss=1008.15  active=37337 feature_norm=46.54\n",
      "Iter 216 time=0.30  loss=1008.04  active=37295 feature_norm=46.56\n",
      "Iter 217 time=0.30  loss=1007.55  active=37262 feature_norm=46.55\n",
      "Iter 218 time=0.30  loss=1007.43  active=37242 feature_norm=46.57\n",
      "Iter 219 time=0.30  loss=1006.95  active=37217 feature_norm=46.57\n",
      "Iter 220 time=0.30  loss=1006.81  active=37162 feature_norm=46.58\n",
      "Iter 221 time=0.30  loss=1006.37  active=37133 feature_norm=46.58\n",
      "Iter 222 time=0.30  loss=1006.25  active=37096 feature_norm=46.59\n",
      "Iter 223 time=0.32  loss=1005.80  active=37086 feature_norm=46.59\n",
      "Iter 224 time=0.32  loss=1005.69  active=37061 feature_norm=46.60\n",
      "Iter 225 time=0.30  loss=1005.23  active=37047 feature_norm=46.60\n",
      "Iter 226 time=0.29  loss=1005.10  active=36986 feature_norm=46.61\n",
      "Iter 227 time=0.30  loss=1004.65  active=36940 feature_norm=46.60\n",
      "Iter 228 time=0.31  loss=1004.50  active=36921 feature_norm=46.61\n",
      "Iter 229 time=0.30  loss=1004.07  active=36846 feature_norm=46.61\n",
      "Iter 230 time=0.29  loss=1003.96  active=36841 feature_norm=46.62\n",
      "Iter 231 time=0.32  loss=1003.50  active=36794 feature_norm=46.61\n",
      "Iter 232 time=0.32  loss=1003.39  active=36775 feature_norm=46.62\n",
      "Iter 233 time=0.30  loss=1002.90  active=36703 feature_norm=46.61\n",
      "Iter 234 time=0.29  loss=1002.76  active=36672 feature_norm=46.62\n",
      "Iter 235 time=0.30  loss=1002.31  active=36633 feature_norm=46.61\n",
      "Iter 236 time=0.31  loss=1002.14  active=36631 feature_norm=46.61\n",
      "Iter 237 time=0.29  loss=1001.71  active=36550 feature_norm=46.60\n",
      "Iter 238 time=0.32  loss=1001.61  active=36553 feature_norm=46.61\n",
      "Iter 239 time=0.30  loss=1001.10  active=36562 feature_norm=46.60\n",
      "Iter 240 time=0.35  loss=1000.97  active=36570 feature_norm=46.60\n",
      "Iter 241 time=0.35  loss=1000.49  active=36521 feature_norm=46.59\n",
      "Iter 242 time=0.35  loss=1000.34  active=36489 feature_norm=46.59\n",
      "Iter 243 time=0.38  loss=999.91   active=36451 feature_norm=46.58\n",
      "Iter 244 time=0.37  loss=999.80   active=36446 feature_norm=46.58\n",
      "Iter 245 time=0.33  loss=999.34   active=36418 feature_norm=46.56\n",
      "Iter 246 time=0.30  loss=999.23   active=36450 feature_norm=46.57\n",
      "Iter 247 time=0.30  loss=998.79   active=36395 feature_norm=46.56\n",
      "Iter 248 time=0.30  loss=998.63   active=36367 feature_norm=46.57\n",
      "Iter 249 time=0.32  loss=998.25   active=36304 feature_norm=46.56\n",
      "Iter 250 time=0.33  loss=998.12   active=36280 feature_norm=46.56\n",
      "Iter 251 time=0.33  loss=997.73   active=36238 feature_norm=46.56\n",
      "Iter 252 time=0.31  loss=997.61   active=36221 feature_norm=46.56\n",
      "Iter 253 time=0.33  loss=997.22   active=36170 feature_norm=46.56\n",
      "Iter 254 time=0.32  loss=997.06   active=36165 feature_norm=46.57\n",
      "Iter 255 time=0.33  loss=996.75   active=36094 feature_norm=46.56\n",
      "Iter 256 time=0.35  loss=996.57   active=36081 feature_norm=46.57\n",
      "Iter 257 time=0.31  loss=996.30   active=36017 feature_norm=46.56\n",
      "Iter 258 time=0.28  loss=996.09   active=36024 feature_norm=46.57\n",
      "Iter 259 time=0.32  loss=995.85   active=36016 feature_norm=46.56\n",
      "Iter 260 time=0.29  loss=995.61   active=36001 feature_norm=46.57\n",
      "Iter 261 time=0.29  loss=995.39   active=35914 feature_norm=46.56\n",
      "Iter 262 time=0.35  loss=995.13   active=35916 feature_norm=46.57\n",
      "Iter 263 time=0.30  loss=994.95   active=35814 feature_norm=46.56\n",
      "Iter 264 time=0.29  loss=994.68   active=35795 feature_norm=46.57\n",
      "Iter 265 time=0.35  loss=994.50   active=35727 feature_norm=46.56\n",
      "Iter 266 time=0.30  loss=994.22   active=35702 feature_norm=46.57\n",
      "Iter 267 time=0.31  loss=994.05   active=35649 feature_norm=46.56\n",
      "Iter 268 time=0.31  loss=993.76   active=35639 feature_norm=46.57\n",
      "Iter 269 time=0.30  loss=993.60   active=35591 feature_norm=46.56\n",
      "Iter 270 time=0.34  loss=993.32   active=35562 feature_norm=46.57\n",
      "Iter 271 time=0.31  loss=993.18   active=35481 feature_norm=46.56\n",
      "Iter 272 time=0.30  loss=992.86   active=35497 feature_norm=46.56\n",
      "Iter 273 time=0.31  loss=992.71   active=35398 feature_norm=46.55\n",
      "Iter 274 time=0.30  loss=992.42   active=35425 feature_norm=46.56\n",
      "Iter 275 time=0.31  loss=992.27   active=35315 feature_norm=46.55\n",
      "Iter 276 time=0.29  loss=991.98   active=35291 feature_norm=46.55\n",
      "Iter 277 time=0.30  loss=991.85   active=35236 feature_norm=46.54\n",
      "Iter 278 time=0.30  loss=991.56   active=35254 feature_norm=46.55\n",
      "Iter 279 time=0.31  loss=991.48   active=35183 feature_norm=46.54\n",
      "Iter 280 time=0.31  loss=991.13   active=35198 feature_norm=46.55\n",
      "Iter 281 time=0.30  loss=990.99   active=35129 feature_norm=46.54\n",
      "Iter 282 time=0.29  loss=990.71   active=35146 feature_norm=46.55\n",
      "Iter 283 time=0.31  loss=990.58   active=35121 feature_norm=46.54\n",
      "Iter 284 time=0.30  loss=990.31   active=35081 feature_norm=46.55\n",
      "Iter 285 time=0.30  loss=990.21   active=34986 feature_norm=46.54\n",
      "Iter 286 time=0.36  loss=989.93   active=34990 feature_norm=46.55\n",
      "Iter 287 time=0.40  loss=989.84   active=34970 feature_norm=46.54\n",
      "Iter 288 time=0.29  loss=989.52   active=34978 feature_norm=46.55\n",
      "Iter 289 time=0.32  loss=989.41   active=34915 feature_norm=46.55\n",
      "Iter 290 time=0.32  loss=989.14   active=34904 feature_norm=46.55\n",
      "Iter 291 time=0.32  loss=989.02   active=34875 feature_norm=46.55\n",
      "Iter 292 time=0.32  loss=988.79   active=34869 feature_norm=46.55\n",
      "Iter 293 time=0.30  loss=988.66   active=34832 feature_norm=46.55\n",
      "Iter 294 time=0.29  loss=988.43   active=34845 feature_norm=46.55\n",
      "Iter 295 time=0.30  loss=988.26   active=34796 feature_norm=46.55\n",
      "Iter 296 time=0.29  loss=988.05   active=34771 feature_norm=46.55\n",
      "Iter 297 time=0.29  loss=987.87   active=34695 feature_norm=46.54\n",
      "Iter 298 time=0.29  loss=987.67   active=34669 feature_norm=46.55\n",
      "Iter 299 time=0.30  loss=987.50   active=34575 feature_norm=46.54\n",
      "Iter 300 time=0.30  loss=987.35   active=34533 feature_norm=46.54\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 98.161\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 34533 (165947)\n",
      "Number of active attributes: 19198 (110772)\n",
      "Number of active labels: 5 (5)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=300,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True)\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.1),\n",
    "    'c2': scipy.stats.expon(scale=0.1),\n",
    "}\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred = crf.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE      0.714     0.669     0.691       661\n",
      "       B-SSI      0.735     0.606     0.664       165\n",
      "        I-AE      0.679     0.659     0.669      1282\n",
      "       I-SSI      0.409     0.281     0.333        64\n",
      "           O      0.951     0.962     0.956     11701\n",
      "\n",
      "    accuracy                          0.912     13873\n",
      "   macro avg      0.698     0.635     0.663     13873\n",
      "weighted avg      0.910     0.912     0.911     13873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flat_classification_report(\n",
    "    y_validation, y_pred, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top likely transitions:\n",
      "B-SSI  -> I-SSI   6.273916\n",
      "I-SSI  -> I-SSI   4.366255\n",
      "B-AE   -> I-AE    3.330324\n",
      "I-AE   -> I-AE    1.921833\n",
      "O      -> O       0.859778\n",
      "B-SSI  -> O       0.127211\n",
      "O      -> B-AE    0.122444\n",
      "I-SSI  -> B-SSI   0.065738\n",
      "O      -> B-SSI   0.050733\n",
      "I-SSI  -> B-AE    -0.132096\n",
      "I-SSI  -> O       -0.241905\n",
      "B-SSI  -> B-AE    -0.315562\n",
      "B-SSI  -> B-SSI   -0.418638\n",
      "I-AE   -> B-SSI   -0.643569\n",
      "I-AE   -> O       -1.108093\n",
      "B-AE   -> I-SSI   -1.373583\n",
      "B-AE   -> B-SSI   -1.476553\n",
      "I-AE   -> B-AE    -1.490102\n",
      "B-AE   -> O       -1.996593\n",
      "I-AE   -> I-SSI   -2.320411\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-SSI  -> O       0.127211\n",
      "O      -> B-AE    0.122444\n",
      "I-SSI  -> B-SSI   0.065738\n",
      "O      -> B-SSI   0.050733\n",
      "I-SSI  -> B-AE    -0.132096\n",
      "I-SSI  -> O       -0.241905\n",
      "B-SSI  -> B-AE    -0.315562\n",
      "B-SSI  -> B-SSI   -0.418638\n",
      "I-AE   -> B-SSI   -0.643569\n",
      "I-AE   -> O       -1.108093\n",
      "B-AE   -> I-SSI   -1.373583\n",
      "B-AE   -> B-SSI   -1.476553\n",
      "I-AE   -> B-AE    -1.490102\n",
      "B-AE   -> O       -1.996593\n",
      "I-AE   -> I-SSI   -2.320411\n",
      "B-AE   -> B-AE    -2.374015\n",
      "B-SSI  -> I-AE    -2.563631\n",
      "I-SSI  -> I-AE    -3.106307\n",
      "O      -> I-SSI   -6.691060\n",
      "O      -> I-AE    -11.229230\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top positive:\n2.270345 I-AE     -1word[:2]:to\n2.020014 O        word[:2]:to\n1.963728 B-AE     tokenizedtext:Insomnia\n1.879700 O        word.istitle()\n1.769548 O        +1:word.lower():(\n1.769548 O        +1:word.stripascii():(\n1.681397 I-AE     word.stripascii():I\n1.672317 B-AE     tokenizedtext:.\n1.652589 B-SSI    word[-2:]:ia\n1.632817 O        word[:3]:med\n1.572424 O        word.POS1freq():CONJ\n1.572424 O        word.POSmin():CONJ\n1.572424 O        word.POSfirst():CONJ\n1.536526 B-AE     word[-2:]:ia\n1.460009 B-AE     tokenizedtext:day.\n1.449378 O        tokenizedtext:Some\n1.449022 O        word.POS1freq():DET\n1.449022 O        word.POSmin():DET\n1.443601 B-AE     tokenizedtext:vertigo\n1.413958 O        word[:2]:wh\n1.403046 B-AE     tokenizedtext:biggest\n1.395429 I-AE     -1word[-3:]:n't\n1.395429 I-AE     -1word[-2:]:'t\n1.389061 I-AE     -1word.POSfirst():DET\n1.345375 B-SSI    tokenizedtext:depressed\n1.340977 B-SSI    tokenizedtext:intrusive\n1.332610 B-SSI    tokenizedtext:depression\n1.319411 B-SSI    tokenizedtext:noticed\n1.313950 B-AE     -2word[:3]:fou\n1.306264 B-AE     word[-3:]:ess\n\nTop negative:\n-1.212311 I-AE     word[:2]:un\n-1.213092 O        -2word[:3]:wan\n-1.214220 I-SSI    word.POSfirst():VERB\n-1.216632 B-AE     word[:3]:the\n-1.218888 O        tokenizedtext:racing\n-1.219547 O        tokenizedtext:Fatigue\n-1.230093 B-AE     -1word.sentimentneu()\n-1.236538 O        word[-3:]:oom\n-1.259291 O        word[:2]:ga\n-1.267424 O        word.lower():anxiety\n-1.282188 O        tokenizedtext:mania\n-1.301170 O        tokenizedtext:Sleepiness\n-1.320886 B-AE     word[:2]:go\n-1.325289 B-SSI    -1word.POSlast():ADV\n-1.333660 B-SSI    tokenizedtext:it.\n-1.360873 O        tokenizedtext:tears\n-1.364331 O        word[:2]:sl\n-1.444372 B-AE     word.POSfirst():PRON\n-1.445679 B-AE     word.POS1freq():PRON\n-1.445679 B-AE     word.POSmin():PRON\n-1.451884 I-SSI    -1word[:2]:an\n-1.502121 B-AE     -1word.sentimentpos()\n-1.503065 B-AE     tokenizedtext:don't\n-1.512387 B-SSI    word.sentimentpos()\n-1.833411 B-AE     -1word.POS4freq():ADJ\n-1.937849 O        +1:word.lower():,\n-1.937849 O        +1:word.stripascii():,\n-2.074611 B-SSI    word.isanxious():no\n-2.241411 O        word[-2:]:ia\n-2.496769 I-AE     word.istitle()\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "source": [
    "## Apply to test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_finaltext, finaltexts = read_file('./TEST_REVIEW_TEXT.txt')\n",
    "\n",
    "finaltexts = finaltexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loweredsent = []\n",
    "for sent in texts:\n",
    "    loweredword = []\n",
    "    for word in sent:\n",
    "        x = word\n",
    "        x = re.sub('[^A-Za-z0-9]+', ' ', x) \n",
    "        loweredword.append(x)\n",
    "    loweredsent.append(loweredword)\n",
    "\n",
    "taggedsent = [nltk.pos_tag(sent, tagset='universal') for sent in loweredsent]\n",
    "flattaggedsent = [item for sublist in taggedsent for item in sublist]\n",
    "\n",
    "\n",
    "corpusdictfromtraining = dict(nltk.ConditionalFreqDist((w.lower(), t) \n",
    "        for w, t in flattaggedsent))\n",
    "\n",
    "\n",
    "corpusdictfromtrainingnofreq = {}\n",
    "for k, v in corpusdictfromtraining.items():\n",
    "    corpusdictfromtrainingnofreq[k] = dict(v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(word, index, tokenizedtext):\n",
    "    strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "\n",
    "    dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "    \n",
    "\n",
    "    features = {\n",
    "        'tokenizedtext': tokenizedtext,\n",
    "        'position': index,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.stripascii()': strippedasciilower,\n",
    "        'word.strippunc()': strippedpunc,\n",
    "        'word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "        'word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "        'word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "        'word[-3:]': strippedpunc[-3:],\n",
    "        'word[-2:]': strippedpunc[-2:],\n",
    "        'word[:2]': strippedpunc[:2],\n",
    "        'word[:3]': strippedpunc[:3],\n",
    "        'word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "        'word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "        'word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "        'word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "        'word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "        'word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "        'word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "        'word.POSfirst()': list(dictionary.items())[0][0],\n",
    "        'word.POSlast()': list(dictionary.items())[-1][0],\n",
    "        'sent.sentimentpos()': float(sia.polarity_scores(\" \".join(tokenizedtext))['pos']),\n",
    "        'sent.sentimentneg()': float(sia.polarity_scores(\" \".join(tokenizedtext))['neg']),\n",
    "        'sent.sentimentneu()': float(sia.polarity_scores(\" \".join(tokenizedtext))['neu']),\n",
    "\n",
    "        'word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "        'word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "        'word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "        'word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "        'word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu']),\n",
    "\n",
    "        'word.isdepress()': 'yes' if strippedpunc in ['depression', 'depressed'] else 'no',\n",
    "        'word.isanxious()': 'yes' if strippedpunc in ['anxiety', 'anxious', 'worry', 'worried'] else 'no',\n",
    "        'word.issuicide()': 'yes' if strippedpunc in ['suicide', 'suicidal', 'kill'] else 'no',\n",
    "        'word.isinsomnia()': 'yes' if strippedpunc in ['insomnia', 'insomniac', 'sleep'] else 'no',\n",
    "        'word.isheadtired()': 'yes' if strippedpunc in ['tired', 'fatigue', 'migraine'] else 'no',\n",
    "        'word.isdelusion()': 'yes' if strippedpunc in ['delusional', 'delusion'] else 'no',\n",
    "        'word.isanger()': 'yes' if strippedpunc in ['anger', 'angry', 'fury', 'furious', 'mad'] else 'no',\n",
    "        'word.isbulimia()': 'yes' if strippedpunc in ['bulimia', 'bulimic'] else 'no',\n",
    "        'word.isafraid()': 'yes' if strippedpunc in ['fear', 'afraid'] else 'no',\n",
    "        'word.isdisorder()': 'yes' if strippedpunc in ['disorder'] else 'no',\n",
    "        'word.isbipolar()': 'yes' if strippedpunc in ['bipolar'] else 'no'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    if index > 0:\n",
    "        word = tokenizedtext[index-1]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '-1:word.lower()': word.lower(),\n",
    "            '-1:word.stripascii()': strippedasciilower,\n",
    "            '-1:word.strippunc()': strippedpunc,\n",
    "            '-1word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '-1word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '-1word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '-1word[-3:]': strippedpunc[-3:],\n",
    "            '-1word[-2:]': strippedpunc[-2:],\n",
    "            '-1word[:2]': strippedpunc[:2],\n",
    "            '-1word[:3]': strippedpunc[:3],\n",
    "            '-1word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '-1word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '-1word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '-1word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '-1word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '-1word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '-1word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '-1word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '-1word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '-1word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '-1word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '-1word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '-1word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '-1word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "\n",
    "    if index > 1:\n",
    "        word = tokenizedtext[index-2]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '-2:word.lower()': word.lower(),\n",
    "            '-2:word.stripascii()': strippedasciilower,\n",
    "            '-2:word.strippunc()': strippedpunc,\n",
    "            '-2word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '-2word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '-2word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '-2word[-3:]': strippedpunc[-3:],\n",
    "            '-2word[-2:]': strippedpunc[-2:],\n",
    "            '-2word[:2]': strippedpunc[:2],\n",
    "            '-2word[:3]': strippedpunc[:3],\n",
    "            '-2word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '-2word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '-2word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '-2word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '-2word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '-2word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '-2word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '-2word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '-2word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '-2word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '-2word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '-2word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '-2word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '-2word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "    \n",
    "    if index < len(tokenizedtext)-1:\n",
    "        word = tokenizedtext[index+1]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '+1:word.lower()': word.lower(),\n",
    "            '+1:word.stripascii()': strippedasciilower,\n",
    "            '+1:word.strippunc()': strippedpunc,\n",
    "            '+1word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '+1word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '+1word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '+1word[-3:]': strippedpunc[-3:],\n",
    "            '+1word[-2:]': strippedpunc[-2:],\n",
    "            '+1word[:2]': strippedpunc[:2],\n",
    "            '+1word[:3]': strippedpunc[:3],\n",
    "            '+1word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '+1word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '+1word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '+1word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '+1word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '+1word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '+1word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '+1word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '+1word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '+1word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '+1word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '+1word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '+1word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '+1word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "    if index < len(tokenizedtext)-2:\n",
    "        \n",
    "        word = tokenizedtext[index+2]\n",
    "        strippedascii = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedasciilower = str(unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "        strippedpunc = re.sub('[^A-Za-z0-9\\']+', '', strippedascii.lower())\n",
    "        dictionary = corpusdictfromtrainingnofreq.get(strippedpunc, dict({'None':1}))\n",
    "\n",
    "\n",
    "        features.update({\n",
    "            '+2:word.lower()': word.lower(),\n",
    "            '+2:word.stripascii()': strippedasciilower,\n",
    "            '+2:word.strippunc()': strippedpunc,\n",
    "            '+2word.istitle()': (re.sub('[^A-Za-z0-9\\']+', '', word)).istitle(),\n",
    "            '+2word.isupper()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isupper(),\n",
    "            '+2word.isdigit()': (re.sub('[^A-Za-z0-9\\']+', '', word)).isdigit(),\n",
    "            '+2word[-3:]': strippedpunc[-3:],\n",
    "            '+2word[-2:]': strippedpunc[-2:],\n",
    "            '+2word[:2]': strippedpunc[:2],\n",
    "            '+2word[:3]': strippedpunc[:3],\n",
    "            '+2word.POS1freq()': nlargest(100, dictionary, key=dictionary.get)[0] if len(dictionary) > 0 else 'None',\n",
    "            '+2word.POS2freq()': nlargest(100, dictionary, key=dictionary.get)[1] if len(dictionary) > 1 else 'None',\n",
    "            '+2word.POS3freq()': nlargest(100, dictionary, key=dictionary.get)[2] if len(dictionary) > 2 else 'None',\n",
    "            '+2word.POS4freq()': nlargest(100, dictionary, key=dictionary.get)[3] if len(dictionary) > 3 else 'None',\n",
    "            '+2word.POS5freq()': nlargest(100, dictionary, key=dictionary.get)[4] if len(dictionary) > 4 else 'None',\n",
    "            '+2word.POS6freq()': nlargest(100, dictionary, key=dictionary.get)[5] if len(dictionary) > 5 else 'None',\n",
    "            '+2word.POSmin()':max(dictionary.items(), key=operator.itemgetter(1))[0],\n",
    "            '+2word.POSfirst()': list(dictionary.items())[0][0],\n",
    "            '+2word.POSlast()': list(dictionary.items())[-1][0],\n",
    "            '+2word.in.sickness()' : strippedpunc in filtered_sicknesses,\n",
    "            '+2word.in.symptoms()' : strippedpunc in filtered_symptoms,\n",
    "            '+2word.sentimentpos()': float(sia.polarity_scores(strippedpunc)['pos']),\n",
    "            '+2word.sentimentneg()': float(sia.polarity_scores(strippedpunc)['neg']),\n",
    "            '+2word.sentimentneu()': float(sia.polarity_scores(strippedpunc)['neu'])\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "def text2features(text):\n",
    "    tokenizedtext = list(text)\n",
    "    return [word2features(i, index, tokenizedtext) for index,i in enumerate(text)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1259.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "004a1b077c5f49228b4b10fe44cc2946"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = [text2features(text) for text in tqdm(finaltexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "foroutput = []\n",
    "\n",
    "for idx,item in enumerate(row_id_finaltext):\n",
    "    withinfo = [item, ' '.join(y_test_pred[idx])]\n",
    "    foroutput.append(withinfo)\n",
    "\n",
    "\n",
    "a = [['ID', 'TAGSEQ']]\n",
    "\n",
    "xout = a +foroutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TEST_REVIEW_LABELSEQ_CRF_run3.txt','w',encoding='utf-8-sig') as out:\n",
    "    for line in xout:\n",
    "        out.write(str('\\t'.join(line)) +'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}