{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Entity Recognition using Deep Learning\n",
    "### Task: Train a deep neural network model using the provided training dataset to identify adverse events and SSI from drug reviews. "
   ]
  },
  {
   "source": [
    "## Import packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Define functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data\n",
    "\n",
    "def reset_random_seeds(x):\n",
    "   os.environ['PYTHONHASHSEED']=str(1)\n",
    "   tf.random.set_seed(x)\n",
    "   np.random.seed(x)\n",
    "   random.seed(x)"
   ]
  },
  {
   "source": [
    "## Read in the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/szoriac/OneDrive/Michigan/=WN 2021/LHS 712/Assignment 3 CRF LSTM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(r\"/Users/szoriac/OneDrive/Michigan/=WN 2021/LHS 712/Assignment 3 CRF LSTM\") \n",
    "print(os.getcwd())\n",
    "\n",
    "row_id_text, texts = read_file('./REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('./REVIEW_LABELSEQ.txt')\n",
    "row_id_finaltext, finaltexts = read_file('./TEST_REVIEW_TEXT.txt')\n",
    "\n",
    "combined_text = texts + finaltexts\n",
    "\n",
    "#For this demo, let's just use the first 100 sentences \n",
    "texts = texts\n",
    "tags = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input representation (converting words to vectors - one hot encoding)"
   ]
  },
  {
   "source": [
    "### Setting up vocabulary of words and tags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set([j for i in combined_text for j in i]))\n",
    "word2idx = {j:i+1 for i,j in enumerate(unique_words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "\n",
    "unique_tags = list(set([j for i in tags for j in i]))\n",
    "label2idx = {j:i for i,j in enumerate(unique_tags)}\n",
    "idx2label = {j:i for i,j in label2idx.items()}\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[j] for j in i] for i in texts]\n",
    "X = pad_sequences(maxlen = 160, sequences = X, padding = \"post\", value = word2idx[\"PAD\"])\n",
    "y = [[label2idx[j] for j in i] for i in tags]\n",
    "y = pad_sequences(maxlen = 160, sequences = y, padding = \"post\", value = label2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes = len(unique_tags)) for i in y]"
   ]
  },
  {
   "source": [
    "### Use Glove pretrained "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = './glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 4296 words (5543 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(word2idx) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "glove_embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tensorflow.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation  = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_26\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_26 (Embedding)     (None, None, 100)         984100    \n_________________________________________________________________\nbidirectional_23 (Bidirectio (None, None, 160)         115840    \n_________________________________________________________________\ndense_64 (Dense)             (None, None, 40)          6440      \n_________________________________________________________________\ndense_65 (Dense)             (None, None, 5)           205       \n=================================================================\nTotal params: 1,106,585\nTrainable params: 122,485\nNon-trainable params: 984,100\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "\n",
    "reset_random_seeds(1)\n",
    "model = Sequential()\n",
    "model.add(glove_embedding_layer)\n",
    "model.add(Bidirectional(LSTM(units=80,return_sequences=True,dropout=0.4), merge_mode = 'concat'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dense(len(label2idx.keys()), activation=\"sigmoid\"))\n",
    "opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run1()\n",
    "#run3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.9882 - accuracy: 0.8718 - val_loss: 0.0693 - val_accuracy: 0.9854\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 14s 405ms/step - loss: 0.0713 - accuracy: 0.9839 - val_loss: 0.0508 - val_accuracy: 0.9855\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 11s 315ms/step - loss: 0.0567 - accuracy: 0.9836 - val_loss: 0.0416 - val_accuracy: 0.9868\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 11s 314ms/step - loss: 0.0484 - accuracy: 0.9851 - val_loss: 0.0358 - val_accuracy: 0.9886\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 11s 303ms/step - loss: 0.0418 - accuracy: 0.9867 - val_loss: 0.0320 - val_accuracy: 0.9898\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 0.0370 - accuracy: 0.9880 - val_loss: 0.0299 - val_accuracy: 0.9905\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 14s 387ms/step - loss: 0.0340 - accuracy: 0.9890 - val_loss: 0.0284 - val_accuracy: 0.9909\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 11s 308ms/step - loss: 0.0329 - accuracy: 0.9895 - val_loss: 0.0272 - val_accuracy: 0.9911\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 11s 306ms/step - loss: 0.0312 - accuracy: 0.9902 - val_loss: 0.0274 - val_accuracy: 0.9919\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 11s 307ms/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 0.0260 - val_accuracy: 0.9920\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 11s 311ms/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.0255 - val_accuracy: 0.9921\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 11s 325ms/step - loss: 0.0297 - accuracy: 0.9908 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 11s 314ms/step - loss: 0.0280 - accuracy: 0.9913 - val_loss: 0.0242 - val_accuracy: 0.9922\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 12s 332ms/step - loss: 0.0266 - accuracy: 0.9916 - val_loss: 0.0242 - val_accuracy: 0.9924\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 12s 351ms/step - loss: 0.0265 - accuracy: 0.9916 - val_loss: 0.0238 - val_accuracy: 0.9923\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 12s 338ms/step - loss: 0.0263 - accuracy: 0.9915 - val_loss: 0.0236 - val_accuracy: 0.9921\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 14s 411ms/step - loss: 0.0257 - accuracy: 0.9920 - val_loss: 0.0235 - val_accuracy: 0.9924\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 14s 404ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.0239 - val_accuracy: 0.9927\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 15s 422ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.0227 - val_accuracy: 0.9925\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 19s 544ms/step - loss: 0.0251 - accuracy: 0.9920 - val_loss: 0.0228 - val_accuracy: 0.9927\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 16s 457ms/step - loss: 0.0247 - accuracy: 0.9919 - val_loss: 0.0232 - val_accuracy: 0.9927\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 16s 474ms/step - loss: 0.0249 - accuracy: 0.9919 - val_loss: 0.0220 - val_accuracy: 0.9924\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 12s 349ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0220 - val_accuracy: 0.9928\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 18s 510ms/step - loss: 0.0238 - accuracy: 0.9921 - val_loss: 0.0220 - val_accuracy: 0.9930\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 14s 398ms/step - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.0230 - val_accuracy: 0.9931\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 15s 420ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0217 - val_accuracy: 0.9928\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 13s 386ms/step - loss: 0.0219 - accuracy: 0.9928 - val_loss: 0.0222 - val_accuracy: 0.9922\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 12s 345ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0219 - val_accuracy: 0.9930\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 14s 407ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.0216 - val_accuracy: 0.9925\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 13s 375ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.0226 - val_accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,np.array(y_train),batch_size=100,epochs=30,validation_split=0.1)\n",
    "\n",
    "y_pred = model.predict(X_validation)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_validation = np.argmax(y_validation, -1)\n",
    "y_pred = [[idx2label[i] for i in row] for row in y_pred]\n",
    "y_validation = [[idx2label[i] for i in row] for row in y_validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopaddingy_pred = [item[:len(texts[idx])] for idx,item in enumerate(y_pred)]\n",
    "nopaddingy_validation = [item[:len(texts[idx])] for idx,item in enumerate(y_validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n        B-AE       0.79      0.66      0.72       583\n       B-SSI       0.78      0.60      0.68       121\n        I-AE       0.81      0.65      0.72       924\n       I-SSI       1.00      0.01      0.03        67\n           O       0.96      0.99      0.98     13133\n\n    accuracy                           0.95     14828\n   macro avg       0.87      0.58      0.63     14828\nweighted avg       0.95      0.95      0.94     14828\n\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(y_pred=nopaddingy_pred, y_true=nopaddingy_validation)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = [[word2idx[j] for j in i] for i in finaltexts]\n",
    "X_final = pad_sequences(maxlen =160, sequences = X_final, padding = \"post\", value = word2idx[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_final)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=-1)\n",
    "y_test_pred = [[idx2label[i] for i in row] for row in y_test_pred]\n",
    "nopaddingy_test_pred = [item[:len(finaltexts[idx])] for idx,item in enumerate(y_test_pred)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "foroutput = []\n",
    "\n",
    "for idx,item in enumerate(row_id_finaltext):\n",
    "    withinfo = [item, ' '.join(nopaddingy_test_pred[idx])]\n",
    "    foroutput.append(withinfo)\n",
    "\n",
    "\n",
    "a = [['ID', 'TAGSEQ']]\n",
    "\n",
    "xout = a +foroutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TEST_REVIEW_LABELSEQ_DEEP_run11.txt','w',encoding='utf-8-sig') as out:\n",
    "    for line in xout:\n",
    "        out.write(str('\\t'.join(line)) +'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}